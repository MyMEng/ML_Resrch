\documentclass[12pt, a4paper, pdflatex]{report}
%  notitlepage - abstract on the same page
\usepackage{indentfirst} % indent frst paragraph of section
\usepackage{fullpage} % full A4 page
\usepackage{amsmath}
\usepackage[pdftex]{graphicx}
\usepackage{cite} % BiTeX
\usepackage{lipsum}
\newcommand{\ts}{\textsuperscript}
\usepackage[usenames,dvipsnames]{color}

\newenvironment{chapabstract}
{
	\vspace{0.5cm}
	\small
	\begin{center}
    \bfseries Chapter Abstract
    \end{center}
}{\vspace{1.5cm}}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

\begin{document}

\begin{titlepage}
\begin{center}
% Upper part of the page. The '~' is needed because \\
% only works if a paragraph has started.
\includegraphics[width=0.5\textwidth]{graphics/UOB-logo.png}~\\[4cm] % was 1cm

% \textsc{\LARGE University of Bristol}\\[1.5cm]

%\textsc{\Large Final year project}\\[0.5cm]

% Title
\HRule \\[0.4cm]
{ \huge \bfseries \emph{Semi-supervised} learning problem.\\
	Comprehensive introduction to the semi-supervised learning with real life application. \\[0.4cm] }
\HRule \\[1.5cm]

% Author and supervisor
\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Authors:}\\
Kacper \textsc{\textbf{Sokol}} \\
Maciej \textsc{\textbf{Kumorek}}
\end{flushleft}
\end{minipage}
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Supervisor:} \\
Peter \textsc{\textbf{Flach}}
\end{flushright}
\end{minipage}

\vfill

% Bottom of the page
{\large \today}
\end{center}
\end{titlepage}

\begin{abstract}
This study aims to present an overview of semi-supervised learning. First part of this paper provides a general introduction to the concept of semi-supervised learning and present a variety of methods and models, their advantages and disadvantages as well as to compare the performance of classification compared to supervised and unsupervised learning methods.
\begin{center}
Keywords: \textbf{semi-supervised, learning...}
\end{center}
\end{abstract}


\newpage
\tableofcontents
\newpage

\chapter{Overview of semi-supervised learning\label{ch:sstheory}}
\section{Introduction}
Usually students taking a machine learning course would be familiar with two main approaches to machine learning. First one is \textit{supervised learning}. Given a set of training data and a classification method we try to predict classes of unseen and unlabelled instances. 

The latter approach, \textit{unsupervised learning}, does not use a labelled set of instances for training purposes or any other apriori knowledge about the data. A common task of this method is \textit{clustering}. We try to "guess" classes our data using a variety of methods and assign unlabelled instances into correct clusters.

However, there are situations when we only have a very limited set of training instances (e.g. due to labelling being very expensive for any reason). If we use classical algorithms involving training, we may not achieve very good results due to limited number of training instances. On the other hand, if we use a clustering method, then we do not take any advantage of having already labelled instances.

\textit{Semi-supervised learning}, is in fact a missing link between these two approaches. Having a limited training set we aim to accurately predict correct classes for unseen data. In this work we would like to explain in details what semi-supervised methods is, what different approaches are and compare it with other available techniques by providing an example of application.

\section{Definition of semi-supervised learning}
In this work we would like to focus on one particular task of semi supervised classification, which builds on top of classical supervised classification task. There are also other settings, such as semi-supervised clustering or dimensionality reductions which are covered in depth by works from other authors.

In semi-supervised classification we aim to find a function $ f $ that will predict classes of unlabelled data with better accuracy than supervised technique using only a very small training set. We extend training set $ X $ so that it contains both kinds of data. In other words, $ X = L \cup U $, where $ L $ is a set of $ n $ instances (denoted by $ x $) with corresponding labels (denoted by $ y $): $ L = \{ (x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\} $ and $ U $ is the set of $ m $ unlabelled data points: $ U = \{ x_{n+1}, x_{n+2}, ..., x_{n+m}\} $. Also, let's call set of $n$ labels or classes only: $ Y = \{ y_1, y_2, ..., y_n \} $

In sections below we would like to define certain assumptions of semi-supervised learning.

\subsection{Inductive and transductive learning}
Partially-supervised learning presents two different approaches to learning: inductive and transductive\cite{chapelle06}. 

Inductive approach involves using a mixture or labelled and unlabelled data to classify an unseen test set of unlabelled instances. In other words we try to find a function: $ f: X_{test} \to Y $ where $ X_{test} $ contains unlabelled points beyond provided training set, i.e. $ X_{test} = \{ x_l, x_{l+1}, ..., x_{k} \} $ where $ k > l > n+m $.

In the latter approach, we focus on classifying data within the training sample. Hence our goal is to classify available unlabelled data deal rather than an external test dataset. Again we can express this as a function: $ f: X_{test} \to Y $, but now $ X_{test} $ contains unlabelled points provided in training set, i.e. $ X_{test} = \{ x_n, x_{n+1}, ..., x_{m} \} $.



\subsection{Smoothness assumption}

Semi-supervised learning assumes smoothness prior for its inputs and outputs\cite{chapelle06}. For instance, in image processing a general smoothness prior assumes that in a given space output changes in a smooth way over distance or time\cite{stan95}. Similarly in machine learning, labels tend to change smoothly over the space. Under assumptions of semi-supervised learning we extend this generalisation to include information about smoothness of inputs. We assume that for two given inputs $ x_1, x_2 $ in the distance between corresponding labels $ l_1, l_2 $ will be smaller if the inputs are in a high density region, but the labels might be separated further away from each other when inputs are placed in lower density cluster.

\subsection{Cluster assumption}

Another assumption we make about the data is that we believe that points tend to form clusters. Furthermore, we assume that if given two points $ x_1, x_2 $ are geometrically within the same cluster, they would share the same class\cite{chapelle06}.

This assumption is rather intuitive -  if data of same class tends to form clusters it is unlikely that a point of a different class will be placed in a "wrong" cluster. Although it is still possible, this generalisation allows us to look at the structure of unlabelled data points and use observation in semi-supervised learning algorithms.

\subsection{Manifold assumption}
 
When performing classification tasks, one of the big problems is data dimensionality. E.g. let's imagine we trained a spam filter that would estimate likelihood of e-mail being spam and non-spam based on certain keywords. In this example each keyword added to the classifier adds another dimension. 

Having many dimensions is a problem known as \textit{curse of dimensionality}\cite{chapelle06}. Both data size grows exponentially with features added, but also correlation between features varies and it becomes harder to find a good estimations for classes. The manifold assumption states that high dimensional data lies on a low-dimensional manifold. What it means is that we may try to reduce number of features used in learning by selecting only features correlated, while we maintain good performance of the classifier. 

 
\section{Models}
\subsection{Generative model}
\subsection{Low-density separation}
\subsection{Graph based method}

\section{Applications of semi-supervised learning}














\chapter{The experiment}

% \section{Sub-Abstract}
\begin{chapabstract}
For experimental part of this assignment we decided to build a simple semi-supervised classifier using algorithms already implemented in \texttt{WEKA} machine learning package. The goal was to learn a model from small subset $A$ of data set $U$ and use all remaining instances $B = U \setminus A$ with removed ground truth $B^{\prime}$ to boost it. With such a classifier we wanted to check whether it will outperform supervised classifier trained only on initial set $A$ and compare it with supervised classifier trained on data set $U$. The measure of classifiers performance used to evaluate results is a number of correctly classified instances out of whole training set (Confusions Matrices are also provided).
\end{chapabstract}

\section{Background}
\lipsum[1]

\section{Introduction}
\lipsum[1]

\section{Experiment design}
% \subsection{General approach}
As explained in \textbf{Chapter~\ref{ch:sstheory}} semi-supervised learning aims at building a classifier from relatively small subset of data which is labeled. To achieve this we randomly extract specified number of labeled data and use it to initially build base classifier. The same sample is used later to train supervised classifier to produce a lower bound on performance.

\subsubsection{Remark: Obtaining predictions}
To avoid methodological bias we decided to use 4 implemented in \texttt{WEKA} classifiers:
\begin{description}
\item[J48 tree] \emph{C4.5 decision tree}--- confidence factor 0.25; the minimum number of objects in the leaf nodes 2; number of folds 3,
\item[IBk] \emph{$k$ nearest neighbors}--- $k$ value equal to 1; linear nearest neighbor search with euclidean distance,
\item[SMO with Polynomial kernel] \emph{sequential minimal optimization algorithm for training a support vector classifier}--- with Polynomial kernel exponent 1,
\item[SMO with RBF kernel] \emph{sequential minimal optimization algorithm for training a support vector classifier}--- with Gamma value 0.01.
\end{description}

The next step is boosting procedure. To this end, the remaining unlabeled instances ($B^{\prime}$) are classified using 4 mentioned algorithms and partitioned into 4 groups according to obtained prediction:\\[-0.6cm]
\begin{center}\textbf{How many classifiers agreed on a prediction:}\end{center}
\begin{tabular}{rp{12cm}}
% \multicolumn{2}{c}{\textbf{How many classifiers agreed on a prediction:}}\\
\textbf{All} & All 4 \texttt{WEKA} classifiers agreed on one label prediction.\\
\textbf{3 out of 4} & 3 classifiers agreed with predicted label. Majority label is selected.\\
\textbf{2 out of 4} & 2 classifiers agreed on common label. In case of 2 pairs of different labels a ``fair coin is tossed'' to select one out of two labels.\\
\textbf{None} & All classifiers predicted different label. One label is chosen at random.\\[0.4cm]
\end{tabular}

While running the experiment it is possible to decide how many instances from the above categories should be used to improve classification. Selected instances are labeled based on above rules and appended to training set. With classifiers trained on expanded set remaining unlabeled instances are are classified and grouped. This process is repeated until person running experiment decides to stop boosting procedure.\\

Final step is to indicate whether testing will be performed on external test set or on already supplied training set. In later case supervised learning trained on a whole set is performed with \emph{10-folds cross-validation}; supervised learning on initial part of train set $A$ is evaluated on whole train set $U$ as well as in semi-supervised case.

\section{The experiment}
\subsection{Used data sets}
\subsection{Approaches to the experiment}



% Then. supervised algorithm as predictors
\newpage
\begin{center} \textbf{\huge ~\vspace{5cm} FIN} \end{center}

\bibliography{references}{}
\bibliographystyle{plain}

\end{document}
